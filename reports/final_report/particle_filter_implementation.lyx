#LyX 1.6.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass IEEEtran
\use_default_options false
\language english
\inputencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\float_placement tbh
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 0
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Subsection
Particle filter
\end_layout

\begin_layout Subsubsection
Java simulation
\end_layout

\begin_layout Standard
Simulations in Java where implemented only to weed out bugs.
 This approach was very successful since the filter, when on the brick,
 worked as intended.
 The filter could also be profiled to find where optimizations would do
 most good.
 Even though profiling did give a better insight on how the filter worked
 in practice the benefits were limited due to the vast differences of hardware
 between a PC and a NXT.
\end_layout

\begin_layout Subsubsection
Brick implementation
\end_layout

\begin_layout Standard
A problem with the particle filter approach was that it is very CPU intensive.
 It was clear from the beginning that some platform and problem specific
 optimizations would be needed.
\end_layout

\begin_layout Standard
A common approach to speeding up non-linear functions is look up tables.
 The up side of these is that they take little time to do.
 The only operations necessary is a rounding to the tables precision and
 then a look up in an array.
 The cons of this approach is that a lot of memory is needed to give the
 table accuracy.
 Since we did not know until very late in the implementation phase how much
 memory would be available, look up tables were not preferred.
\end_layout

\begin_layout Standard
The steps who needed optimizations badly 
\begin_inset CommandInset citation
LatexCommand cite
key "MiodragBolic2004-11-12"

\end_inset

 where:
\end_layout

\begin_layout Enumerate
Some mathematical functions would be to slow on the NXT.
\end_layout

\begin_deeper
\begin_layout Enumerate
When comparing particles with sensor data one arc-tan operation is needed
 per particle (i.e.
 
\begin_inset Formula $\theta_{error}=\theta_{sighting}-arctan(\frac{y_{particle}-y_{est.position}}{x_{particle}-x_{est.position}})$
\end_inset

).
\end_layout

\begin_layout Enumerate
Criterion function is a Gaussian PDF (i.e.
 
\begin_inset Formula $w=C_{1}e^{C_{2}\theta_{error}^{2}}$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Enumerate
Re-sampling needs one random sample per state variable and particle from
 a probability distribution in each iteration.
\end_layout

\begin_layout Enumerate
Floating point precision is not native to the NXT but implemented in firmware
\end_layout

\begin_layout Standard

\series bold
Since the handling of floating point
\series default
 arithmetic is not native to the NXT, but emulated in the firmware, a classic
 approach to embedded mathematics was used, the so called fixed point arithmetic
 
\begin_inset CommandInset citation
LatexCommand cite
key "Yates2009-07-07"

\end_inset

.
 Fixed point uses only integer operation which are much faster than floating
 point, usually they are even so on processors with native floating point
 support.
 The basic idea is that all numbers are multiplied by a constant and then
 truncating the floating point number into an integer.
 If the constant is chosen as a power of two many important operations collapse
 to a bit shift.
 When doing a multiplication the result looks like the following.
\end_layout

\begin_layout Standard
\begin_inset Formula $c=((a\cdot d)\cdot(b\cdot d))/d$
\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $d$
\end_inset

 is the constant all numbers are multiplied by.
 Now if 
\begin_inset Formula $d$
\end_inset

 is chosen as 
\begin_inset Formula $2^{BASE}$
\end_inset

 then and all numbers are represented as fixed point at all times the above
 equation collapses to:
\end_layout

\begin_layout Standard
\begin_inset Formula $c=(a\cdot b)\gg BASE$
\end_inset


\end_layout

\begin_layout Standard
Where the symbol 
\begin_inset Formula $\gg$
\end_inset

 represents a bit shift effectively dividing with the factor 
\begin_inset Formula $2^{BASE}$
\end_inset

.
 The base used in the implementation was 20 and the format is called 12.20
 fixed point.
\end_layout

\begin_layout Standard
When normalizing vector represented in fixed point some kind of square root
 algorithm was needed.
 Since floating point is used on many embedded devices there have been a
 lot of research on different algorithms.
 A simple algorithm which searches for the right square was implemented
 
\begin_inset CommandInset citation
LatexCommand cite
key "Turkowski1994"

\end_inset

 and modified to suit the fixed point setup.
\end_layout

\begin_layout Standard
For sine and cosine a simple look up table was used.
 Because of symmetry in and between the two functions a high resolution
 can be obtained using little memory.
 
\end_layout

\begin_layout Standard

\series bold
To replace the arc-tan and the exponential 
\series default
function a linearization technique was investigated.
 The data given are coordinates on a plane which lead to the though of the
 definition of inner product in 
\begin_inset Formula $\mathbb{R}^{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
u\cdot v=\frac{cos\theta}{\left\Vert u\right\Vert \left\Vert v\right\Vert }\Longrightarrow\theta=acos\left(\frac{u\cdot v}{\left\Vert u\right\Vert \left\Vert v\right\Vert }\right)\label{eq:Particle vector product}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
w=f_{weight}\left(\frac{v_{x}}{\left\Vert v\right\Vert }\right)\end{equation}

\end_inset


\end_layout

\begin_layout Standard
If the evaluation function was plotted against 
\begin_inset Formula $cos\theta$
\end_inset

 from the vector product as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Particle vector product"

\end_inset

 the result could be approximated with a piecewise linear function.
 After some work trying to find the best segments to linearize the resulting
 function is plotted against angular deviation in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:particle_weight_function"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/particle_filter_java_penalty_function.eps
	lyxscale 50
	scale 50

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:particle_weight_function"

\end_inset

Plot of the linearized weight function and analytical values of the same
 as reference
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Re-sampling the particles
\series default
 is not a hard or heavy task but can still be improved significantly.
 Most of the time the particles are good and the number of particles which
 really need to be re-sampled is small.
 If only the worst particles are re-sampled the filter would converge slower
 but would not need to do so much random sampling.
 However this method might need some overhead in sorting the particles.
 The method worked but was never compared with respect to accuracy.
\end_layout

\end_body
\end_document
